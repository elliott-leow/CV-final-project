{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scooter Bump Detection & Prediction System\n",
        "This notebook detects bumps using vertical optical flow, then trains a model to predict bumps before they occur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#imports and setup\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
          ]
        }
      ],
      "source": [
        "#imports and setup\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from scipy import signal\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#ml imports\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#config\n",
        "DATA_DIR = Path(\"data-scaled\")\n",
        "OUTPUT_DIR = Path(\"output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "FPS = 20  #target fps after scaling\n",
        "SEGMENT_LENGTH = 10  #frames per segment\n",
        "BUMP_THRESHOLD_PERCENTILE = 95  #percentile for bump detection\n",
        "LOOKAHEAD_FRAMES = 10  #how many frames ahead to look for prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Load Video and Compute Optical Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_video(video_path):\n",
        "    \"\"\"load video frames as numpy array\"\"\"\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    frames = []\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "    \n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def compute_optical_flow(frames):\n",
        "    \"\"\"compute dense optical flow between consecutive frames\"\"\"\n",
        "    grays = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in frames]\n",
        "    flows = []\n",
        "    \n",
        "    for i in range(len(grays) - 1):\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            grays[i], grays[i + 1],\n",
        "            None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "        flows.append(flow)\n",
        "        \n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"processed {i + 1}/{len(grays) - 1} frames\")\n",
        "    \n",
        "    return flows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#road hazard detection and tracking classes\n",
        "class RoadHazardDetector:\n",
        "    \"\"\"detects potential bump-causing features on road surface\"\"\"\n",
        "    \n",
        "    def __init__(self, roi_top_ratio=0.4, roi_bottom_ratio=0.95):\n",
        "        self.roi_top = roi_top_ratio\n",
        "        self.roi_bottom = roi_bottom_ratio\n",
        "        \n",
        "    def detect_hazards(self, frame):\n",
        "        \"\"\"detect potential road hazards (cracks, holes, bumps, debris)\"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n",
        "        h, w = gray.shape\n",
        "        \n",
        "        roi_y1 = int(h * self.roi_top)\n",
        "        roi_y2 = int(h * self.roi_bottom)\n",
        "        roi = gray[roi_y1:roi_y2, :]\n",
        "        roi_h = roi_y2 - roi_y1\n",
        "        \n",
        "        hazards = []\n",
        "        \n",
        "        #edge-based detection (cracks, edges)\n",
        "        edges = cv2.Canny(roi, 30, 100)\n",
        "        \n",
        "        #local contrast anomalies (dark spots = holes)\n",
        "        blur = cv2.GaussianBlur(roi, (21, 21), 0)\n",
        "        local_diff = cv2.absdiff(roi, blur)\n",
        "        _, anomaly_mask = cv2.threshold(local_diff, 15, 255, cv2.THRESH_BINARY)\n",
        "        \n",
        "        #texture roughness via laplacian\n",
        "        lap = cv2.Laplacian(roi, cv2.CV_64F)\n",
        "        lap_abs = np.abs(lap).astype(np.uint8)\n",
        "        _, rough_mask = cv2.threshold(lap_abs, 20, 255, cv2.THRESH_BINARY)\n",
        "        \n",
        "        #combine and cleanup\n",
        "        combined = cv2.bitwise_or(edges, anomaly_mask)\n",
        "        combined = cv2.bitwise_or(combined, rough_mask)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "        combined = cv2.morphologyEx(combined, cv2.MORPH_CLOSE, kernel)\n",
        "        combined = cv2.morphologyEx(combined, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        contours, _ = cv2.findContours(combined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        for cnt in contours:\n",
        "            area = cv2.contourArea(cnt)\n",
        "            if area < 50 or area > w * roi_h * 0.3:\n",
        "                continue\n",
        "            x, y, bw, bh = cv2.boundingRect(cnt)\n",
        "            cx, cy = x + bw // 2, y + bh // 2\n",
        "            hazard_roi = roi[max(0,y-5):min(roi_h,y+bh+5), max(0,x-5):min(w,x+bw+5)]\n",
        "            if hazard_roi.size == 0:\n",
        "                continue\n",
        "            hazard = {\n",
        "                'x': cx, 'y': cy + roi_y1,\n",
        "                'width': bw, 'height': bh, 'area': area,\n",
        "                'intensity_mean': hazard_roi.mean(),\n",
        "                'intensity_std': hazard_roi.std(),\n",
        "                'edge_density': edges[y:y+bh, x:x+bw].mean() / 255 if bh > 0 and bw > 0 else 0,\n",
        "                'y_normalized': (cy + roi_y1) / h,\n",
        "                'x_normalized': cx / w,\n",
        "            }\n",
        "            hazards.append(hazard)\n",
        "        return hazards, combined, roi_y1\n",
        "\n",
        "class HazardTracker:\n",
        "    \"\"\"tracks hazards as they approach (move down in frame)\"\"\"\n",
        "    \n",
        "    def __init__(self, max_age=15, min_hits=3):\n",
        "        self.max_age = max_age\n",
        "        self.min_hits = min_hits\n",
        "        self.tracks = []\n",
        "        self.track_id = 0\n",
        "        self.completed = []\n",
        "        \n",
        "    def update(self, hazards, frame_idx):\n",
        "        #predict positions (hazards move DOWN in frame)\n",
        "        for t in self.tracks:\n",
        "            if t['positions'] and len(t['positions']) >= 2:\n",
        "                dy = t['positions'][-1]['y'] - t['positions'][-2]['y']\n",
        "                dx = t['positions'][-1]['x'] - t['positions'][-2]['x']\n",
        "                t['pred_y'] = t['positions'][-1]['y'] + dy\n",
        "                t['pred_x'] = t['positions'][-1]['x'] + dx\n",
        "                t['vel_y'], t['vel_x'] = dy, dx\n",
        "            elif t['positions']:\n",
        "                t['pred_y'] = t['positions'][-1]['y'] + 2\n",
        "                t['pred_x'] = t['positions'][-1]['x']\n",
        "                t['vel_y'], t['vel_x'] = 2, 0\n",
        "        \n",
        "        matched_t, matched_h = set(), set()\n",
        "        for i, h in enumerate(hazards):\n",
        "            best_j, best_d = None, float('inf')\n",
        "            for j, t in enumerate(self.tracks):\n",
        "                if j in matched_t or 'pred_y' not in t:\n",
        "                    continue\n",
        "                d = np.sqrt((h['x']-t['pred_x'])**2 + (h['y']-t['pred_y'])**2)\n",
        "                if t['positions'] and h['y'] < t['positions'][-1]['y'] - 5:\n",
        "                    continue\n",
        "                if d < 50 and d < best_d:\n",
        "                    best_d, best_j = d, j\n",
        "            if best_j is not None:\n",
        "                matched_t.add(best_j)\n",
        "                matched_h.add(i)\n",
        "                t = self.tracks[best_j]\n",
        "                t['positions'].append({'x': h['x'], 'y': h['y'], 'frame': frame_idx, 'features': h})\n",
        "                t['hits'] += 1\n",
        "                t['age'] = 0\n",
        "                t['last_seen'] = frame_idx\n",
        "        \n",
        "        for i, h in enumerate(hazards):\n",
        "            if i not in matched_h:\n",
        "                self.tracks.append({\n",
        "                    'id': self.track_id, 'positions': [{'x': h['x'], 'y': h['y'], 'frame': frame_idx, 'features': h}],\n",
        "                    'hits': 1, 'age': 0, 'start_frame': frame_idx, 'last_seen': frame_idx,\n",
        "                    'vel_y': 0, 'vel_x': 0, 'caused_bump': False, 'bump_frame': None\n",
        "                })\n",
        "                self.track_id += 1\n",
        "        \n",
        "        new_tracks = []\n",
        "        for j, t in enumerate(self.tracks):\n",
        "            if j not in matched_t:\n",
        "                t['age'] += 1\n",
        "            if t['age'] < self.max_age:\n",
        "                new_tracks.append(t)\n",
        "            elif t['hits'] >= self.min_hits:\n",
        "                self.completed.append(t)\n",
        "        self.tracks = new_tracks\n",
        "        return [t for t in self.tracks if t['hits'] >= self.min_hits]\n",
        "    \n",
        "    def get_all_tracks(self):\n",
        "        return self.tracks + self.completed\n",
        "    \n",
        "    def label_bumps(self, bump_frames, lookahead=10):\n",
        "        for t in self.get_all_tracks():\n",
        "            if not t['positions']:\n",
        "                continue\n",
        "            last_f = t['positions'][-1]['frame']\n",
        "            for bf in bump_frames:\n",
        "                if last_f <= bf <= last_f + lookahead:\n",
        "                    t['caused_bump'] = True\n",
        "                    t['bump_frame'] = bf\n",
        "                    break\n",
        "        return self.get_all_tracks()\n",
        "\n",
        "print(\"hazard detection classes defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load videos from data-scaled\n",
        "video_files = list(DATA_DIR.glob(\"*.mp4\")) + list(DATA_DIR.glob(\"*.mov\"))\n",
        "print(f\"found {len(video_files)} video(s): {[v.name for v in video_files]}\")\n",
        "\n",
        "#load first video for processing\n",
        "if video_files:\n",
        "    video_path = video_files[0]\n",
        "    print(f\"\\nloading {video_path.name}...\")\n",
        "    frames = load_video(video_path)\n",
        "    print(f\"loaded {len(frames)} frames, shape: {frames[0].shape}\")\n",
        "else:\n",
        "    print(\"no videos found! run scale_videos.py first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#compute optical flow\n",
        "print(\"computing optical flow...\")\n",
        "flows = compute_optical_flow(frames)\n",
        "print(f\"computed {len(flows)} flow fields\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run hazard detection and tracking on all frames\n",
        "print(\"detecting and tracking road hazards...\")\n",
        "hazard_detector = RoadHazardDetector(roi_top_ratio=0.35, roi_bottom_ratio=0.95)\n",
        "hazard_tracker = HazardTracker(max_age=20, min_hits=3)\n",
        "\n",
        "all_hazards = []\n",
        "all_active_tracks = []\n",
        "frame_h, frame_w = frames[0].shape[:2]\n",
        "\n",
        "for i, frame in enumerate(frames):\n",
        "    hazards, mask, roi_y1 = hazard_detector.detect_hazards(frame)\n",
        "    active = hazard_tracker.update(hazards, i)\n",
        "    \n",
        "    all_hazards.append(hazards)\n",
        "    all_active_tracks.append([dict(t) for t in active])\n",
        "    \n",
        "    if (i + 1) % 100 == 0:\n",
        "        approaching = len([t for t in active if t.get('vel_y', 0) > 1])\n",
        "        print(f\"frame {i+1}/{len(frames)}, hazards: {len(hazards)}, tracks: {len(active)}, approaching: {approaching}\")\n",
        "\n",
        "print(f\"\\ntotal tracks created: {hazard_tracker.track_id}\")\n",
        "print(f\"confirmed tracks (3+ hits): {len([t for t in hazard_tracker.get_all_tracks() if t['hits'] >= 3])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Detect Bumps from Vertical Optical Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_flow_features(flows):\n",
        "    \"\"\"extract vertical flow statistics per frame\"\"\"\n",
        "    features = {\n",
        "        'avg_vy': [],      #average vertical flow\n",
        "        'std_vy': [],      #std of vertical flow\n",
        "        'max_vy': [],      #max vertical flow\n",
        "        'min_vy': [],      #min vertical flow\n",
        "        'avg_vx': [],      #average horizontal flow\n",
        "        'magnitude': [],   #average flow magnitude\n",
        "        'bottom_vy': [],   #vertical flow in bottom half (closer to scooter)\n",
        "    }\n",
        "    \n",
        "    for flow in flows:\n",
        "        vx, vy = flow[..., 0], flow[..., 1]\n",
        "        h = flow.shape[0]\n",
        "        \n",
        "        features['avg_vy'].append(vy.mean())\n",
        "        features['std_vy'].append(vy.std())\n",
        "        features['max_vy'].append(vy.max())\n",
        "        features['min_vy'].append(vy.min())\n",
        "        features['avg_vx'].append(vx.mean())\n",
        "        features['magnitude'].append(np.sqrt(vx**2 + vy**2).mean())\n",
        "        features['bottom_vy'].append(vy[h//2:, :].mean())  #bottom half\n",
        "    \n",
        "    return {k: np.array(v) for k, v in features.items()}\n",
        "\n",
        "flow_features = extract_flow_features(flows)\n",
        "print(f\"extracted features for {len(flow_features['avg_vy'])} frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot vertical optical flow over time\n",
        "time = np.arange(len(flow_features['avg_vy'])) / FPS\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 8), sharex=True)\n",
        "\n",
        "axes[0].plot(time, flow_features['avg_vy'], 'b-', alpha=0.7, label='avg vertical flow')\n",
        "axes[0].set_ylabel('avg vy (pixels/frame)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(time, flow_features['std_vy'], 'r-', alpha=0.7, label='std vertical flow')\n",
        "axes[1].set_ylabel('std vy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(time, flow_features['magnitude'], 'g-', alpha=0.7, label='flow magnitude')\n",
        "axes[2].set_ylabel('magnitude')\n",
        "axes[2].set_xlabel('time (s)')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('optical flow analysis over time')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_bumps(flow_features, threshold_percentile=95, min_distance=5):\n",
        "    \"\"\"detect bumps from sudden changes in vertical flow\"\"\"\n",
        "    avg_vy = flow_features['avg_vy']\n",
        "    \n",
        "    #compute derivative (change in vertical flow)\n",
        "    vy_diff = np.abs(np.diff(avg_vy))\n",
        "    \n",
        "    #also consider absolute flow magnitude as bump indicator\n",
        "    vy_abs = np.abs(avg_vy[1:])  #align with diff\n",
        "    \n",
        "    #combine signals: sudden change + high magnitude\n",
        "    bump_signal = vy_diff + 0.5 * vy_abs\n",
        "    \n",
        "    #smooth slightly to reduce noise\n",
        "    bump_signal_smooth = gaussian_filter1d(bump_signal, sigma=1)\n",
        "    \n",
        "    #threshold based on percentile\n",
        "    threshold = np.percentile(bump_signal_smooth, threshold_percentile)\n",
        "    \n",
        "    #find peaks above threshold\n",
        "    peaks, properties = signal.find_peaks(\n",
        "        bump_signal_smooth, \n",
        "        height=threshold,\n",
        "        distance=min_distance\n",
        "    )\n",
        "    \n",
        "    #offset by 1 since diff reduces length\n",
        "    bump_frames = peaks + 1\n",
        "    bump_intensities = properties['peak_heights']\n",
        "    \n",
        "    return bump_frames, bump_intensities, bump_signal_smooth, threshold\n",
        "\n",
        "bump_frames, bump_intensities, bump_signal, bump_threshold = detect_bumps(\n",
        "    flow_features, \n",
        "    threshold_percentile=BUMP_THRESHOLD_PERCENTILE\n",
        ")\n",
        "\n",
        "print(f\"detected {len(bump_frames)} bumps\")\n",
        "print(f\"bump threshold: {bump_threshold:.3f}\")\n",
        "print(f\"bump frames: {bump_frames[:20]}...\" if len(bump_frames) > 20 else f\"bump frames: {bump_frames}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize bump detection\n",
        "time_signal = np.arange(len(bump_signal)) / FPS\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.plot(time_signal, bump_signal, 'b-', alpha=0.7, label='bump signal')\n",
        "plt.axhline(y=bump_threshold, color='r', linestyle='--', label=f'threshold ({BUMP_THRESHOLD_PERCENTILE}th percentile)')\n",
        "\n",
        "#mark detected bumps\n",
        "bump_times = (bump_frames - 1) / FPS  #offset to align with signal\n",
        "plt.scatter(bump_times, bump_intensities, c='red', s=100, marker='v', label='detected bumps', zorder=5)\n",
        "\n",
        "plt.xlabel('time (s)')\n",
        "plt.ylabel('bump signal intensity')\n",
        "plt.title('bump detection from vertical optical flow')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create frame-level labels (1 = bump, 0 = no bump)\n",
        "num_frames = len(frames)\n",
        "bump_labels = np.zeros(num_frames, dtype=int)\n",
        "\n",
        "#mark bump frames and nearby frames (Â±2 frames window)\n",
        "for bf in bump_frames:\n",
        "    start = max(0, bf - 2)\n",
        "    end = min(num_frames, bf + 3)\n",
        "    bump_labels[start:end] = 1\n",
        "\n",
        "print(f\"total frames: {num_frames}\")\n",
        "print(f\"bump frames: {bump_labels.sum()} ({100*bump_labels.mean():.1f}%)\")\n",
        "print(f\"non-bump frames: {num_frames - bump_labels.sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#label tracks that caused bumps and extract features\n",
        "print(\"labeling bump-causing tracks...\")\n",
        "labeled_tracks = hazard_tracker.label_bumps(bump_frames, lookahead=15)\n",
        "\n",
        "#count stats\n",
        "bump_tracks = [t for t in labeled_tracks if t['caused_bump'] and t['hits'] >= 3]\n",
        "non_bump_tracks = [t for t in labeled_tracks if not t['caused_bump'] and t['hits'] >= 3]\n",
        "print(f\"bump-causing tracks: {len(bump_tracks)}\")\n",
        "print(f\"non-bump tracks: {len(non_bump_tracks)}\")\n",
        "\n",
        "def extract_track_features(track):\n",
        "    \"\"\"extract features from a tracked hazard for classification\"\"\"\n",
        "    positions = track['positions']\n",
        "    if len(positions) < 3:\n",
        "        return None\n",
        "    \n",
        "    features = {}\n",
        "    \n",
        "    #track duration and persistence\n",
        "    features['track_duration'] = positions[-1]['frame'] - positions[0]['frame']\n",
        "    features['track_hits'] = track['hits']\n",
        "    \n",
        "    #trajectory features (how it approached)\n",
        "    y_vals = [p['y'] for p in positions]\n",
        "    x_vals = [p['x'] for p in positions]\n",
        "    \n",
        "    #total vertical travel (should be positive = moving down/approaching)\n",
        "    features['total_dy'] = y_vals[-1] - y_vals[0]\n",
        "    features['total_dx'] = abs(x_vals[-1] - x_vals[0])\n",
        "    \n",
        "    #average velocity\n",
        "    features['avg_vel_y'] = features['total_dy'] / (len(positions) - 1) if len(positions) > 1 else 0\n",
        "    features['avg_vel_x'] = features['total_dx'] / (len(positions) - 1) if len(positions) > 1 else 0\n",
        "    \n",
        "    #velocity consistency (low std = consistent approach)\n",
        "    if len(positions) >= 3:\n",
        "        vel_y = [y_vals[i+1] - y_vals[i] for i in range(len(y_vals)-1)]\n",
        "        features['vel_y_std'] = np.std(vel_y)\n",
        "        features['vel_y_max'] = max(vel_y)\n",
        "    else:\n",
        "        features['vel_y_std'] = 0\n",
        "        features['vel_y_max'] = features['avg_vel_y']\n",
        "    \n",
        "    #where hazard started and ended (normalized 0-1)\n",
        "    features['start_y_norm'] = positions[0]['y'] / frame_h\n",
        "    features['end_y_norm'] = positions[-1]['y'] / frame_h\n",
        "    features['start_x_norm'] = positions[0]['x'] / frame_w\n",
        "    features['end_x_norm'] = positions[-1]['x'] / frame_w\n",
        "    \n",
        "    #hazard is in the center path? (x near 0.5 is more dangerous)\n",
        "    center_dist = [abs(p['x'] / frame_w - 0.5) for p in positions]\n",
        "    features['min_center_dist'] = min(center_dist)\n",
        "    features['avg_center_dist'] = np.mean(center_dist)\n",
        "    \n",
        "    #visual features from first and last detection\n",
        "    first_f = positions[0]['features']\n",
        "    last_f = positions[-1]['features']\n",
        "    \n",
        "    features['first_area'] = first_f['area']\n",
        "    features['last_area'] = last_f['area']\n",
        "    features['area_growth'] = last_f['area'] / (first_f['area'] + 1)  #grows as approaches\n",
        "    \n",
        "    features['first_intensity'] = first_f['intensity_mean']\n",
        "    features['last_intensity'] = last_f['intensity_mean']\n",
        "    features['intensity_change'] = last_f['intensity_mean'] - first_f['intensity_mean']\n",
        "    \n",
        "    features['first_edge_density'] = first_f['edge_density']\n",
        "    features['last_edge_density'] = last_f['edge_density']\n",
        "    \n",
        "    #aggregate features across track\n",
        "    areas = [p['features']['area'] for p in positions]\n",
        "    intensities = [p['features']['intensity_mean'] for p in positions]\n",
        "    edges = [p['features']['edge_density'] for p in positions]\n",
        "    \n",
        "    features['max_area'] = max(areas)\n",
        "    features['avg_intensity'] = np.mean(intensities)\n",
        "    features['intensity_std'] = np.std(intensities)\n",
        "    features['max_edge_density'] = max(edges)\n",
        "    features['avg_edge_density'] = np.mean(edges)\n",
        "    \n",
        "    return features\n",
        "\n",
        "#create training data from tracks\n",
        "print(\"\\ncreating training data from tracked hazards...\")\n",
        "X_tracks = []\n",
        "y_tracks = []\n",
        "track_ids = []\n",
        "\n",
        "for track in labeled_tracks:\n",
        "    if track['hits'] < 3:\n",
        "        continue\n",
        "    features = extract_track_features(track)\n",
        "    if features is None:\n",
        "        continue\n",
        "    \n",
        "    #only include tracks that actually approached (moved downward)\n",
        "    if features['total_dy'] < 5:  #min 5 pixels of downward movement\n",
        "        continue\n",
        "    \n",
        "    X_tracks.append(list(features.values()))\n",
        "    y_tracks.append(1 if track['caused_bump'] else 0)\n",
        "    track_ids.append(track['id'])\n",
        "\n",
        "X_tracks = np.array(X_tracks)\n",
        "y_tracks = np.array(y_tracks)\n",
        "feature_names_tracks = list(features.keys())\n",
        "\n",
        "print(f\"\\ntrack-based training data: X={X_tracks.shape}, y={y_tracks.shape}\")\n",
        "print(f\"positive (bump-causing): {y_tracks.sum()} ({100*y_tracks.mean():.1f}%)\")\n",
        "print(f\"negative (non-bump): {(1-y_tracks).sum()}\")\n",
        "print(f\"features: {len(feature_names_tracks)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train classifier on track features\n",
        "print(\"training hazard bump classifier...\")\n",
        "\n",
        "if len(X_tracks) > 10 and y_tracks.sum() > 0:\n",
        "    #split data\n",
        "    X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "        X_tracks, y_tracks, test_size=0.2, random_state=42, \n",
        "        stratify=y_tracks if y_tracks.sum() >= 2 else None\n",
        "    )\n",
        "    \n",
        "    #scale features\n",
        "    scaler_tracks = StandardScaler()\n",
        "    X_train_ts = scaler_tracks.fit_transform(X_train_t)\n",
        "    X_test_ts = scaler_tracks.transform(X_test_t)\n",
        "    \n",
        "    #class weights for imbalance\n",
        "    if len(np.unique(y_train_t)) > 1:\n",
        "        cw = compute_class_weight('balanced', classes=np.unique(y_train_t), y=y_train_t)\n",
        "        cw_dict = {i: w for i, w in enumerate(cw)}\n",
        "    else:\n",
        "        cw_dict = {0: 1.0, 1: 1.0}\n",
        "    print(f\"class weights: {cw_dict}\")\n",
        "    \n",
        "    #train random forest on track features\n",
        "    rf_tracks = RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=8, min_samples_split=3,\n",
        "        class_weight=cw_dict, random_state=42, n_jobs=-1\n",
        "    )\n",
        "    rf_tracks.fit(X_train_ts, y_train_t)\n",
        "    \n",
        "    #evaluate\n",
        "    y_pred_t = rf_tracks.predict(X_test_ts)\n",
        "    print(\"\\ntrack classifier evaluation:\")\n",
        "    print(classification_report(y_test_t, y_pred_t, target_names=['no bump', 'bump'], zero_division=0))\n",
        "    \n",
        "    #feature importance\n",
        "    importances = rf_tracks.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    \n",
        "    print(\"\\ntop 10 features for bump prediction:\")\n",
        "    for i in range(min(10, len(feature_names_tracks))):\n",
        "        print(f\"  {feature_names_tracks[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "else:\n",
        "    print(\"not enough training data for track classifier\")\n",
        "    rf_tracks = None\n",
        "    scaler_tracks = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ApproachingHazardPredictor:\n",
        "    \"\"\"real-time bump prediction based on approaching hazards\"\"\"\n",
        "    \n",
        "    def __init__(self, model, scaler, feature_names, frame_shape):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.feature_names = feature_names\n",
        "        self.frame_h, self.frame_w = frame_shape[:2]\n",
        "        \n",
        "        self.detector = RoadHazardDetector(roi_top_ratio=0.35, roi_bottom_ratio=0.95)\n",
        "        self.tracker = HazardTracker(max_age=20, min_hits=3)\n",
        "        self.frame_idx = 0\n",
        "        \n",
        "        self.alert_active = False\n",
        "        self.alert_probability = 0.0\n",
        "        self.dangerous_tracks = []\n",
        "        \n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"process frame and predict if approaching hazard will cause bump\"\"\"\n",
        "        hazards, mask, roi_y1 = self.detector.detect_hazards(frame)\n",
        "        active_tracks = self.tracker.update(hazards, self.frame_idx)\n",
        "        \n",
        "        #check each active approaching track\n",
        "        max_prob = 0.0\n",
        "        dangerous = []\n",
        "        \n",
        "        for track in active_tracks:\n",
        "            #only evaluate tracks that are actively approaching\n",
        "            if track.get('vel_y', 0) < 1 or len(track['positions']) < 3:\n",
        "                continue\n",
        "                \n",
        "            #extract features and predict\n",
        "            features = self._extract_live_features(track)\n",
        "            if features is None:\n",
        "                continue\n",
        "            \n",
        "            X = np.array([list(features.values())])\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "            prob = self.model.predict_proba(X_scaled)[0, 1]\n",
        "            \n",
        "            if prob > 0.3:  #moderate confidence threshold\n",
        "                dangerous.append({\n",
        "                    'track': track,\n",
        "                    'probability': prob,\n",
        "                    'position': (track['positions'][-1]['x'], track['positions'][-1]['y'])\n",
        "                })\n",
        "                max_prob = max(max_prob, prob)\n",
        "        \n",
        "        self.dangerous_tracks = dangerous\n",
        "        self.alert_probability = max_prob\n",
        "        self.alert_active = max_prob > 0.5\n",
        "        self.frame_idx += 1\n",
        "        \n",
        "        return self.alert_active, self.alert_probability, dangerous\n",
        "    \n",
        "    def _extract_live_features(self, track):\n",
        "        \"\"\"extract features from live track (same as training)\"\"\"\n",
        "        positions = track['positions']\n",
        "        if len(positions) < 3:\n",
        "            return None\n",
        "        \n",
        "        features = {}\n",
        "        \n",
        "        features['track_duration'] = positions[-1]['frame'] - positions[0]['frame']\n",
        "        features['track_hits'] = track['hits']\n",
        "        \n",
        "        y_vals = [p['y'] for p in positions]\n",
        "        x_vals = [p['x'] for p in positions]\n",
        "        \n",
        "        features['total_dy'] = y_vals[-1] - y_vals[0]\n",
        "        features['total_dx'] = abs(x_vals[-1] - x_vals[0])\n",
        "        features['avg_vel_y'] = features['total_dy'] / (len(positions) - 1) if len(positions) > 1 else 0\n",
        "        features['avg_vel_x'] = features['total_dx'] / (len(positions) - 1) if len(positions) > 1 else 0\n",
        "        \n",
        "        if len(positions) >= 3:\n",
        "            vel_y = [y_vals[i+1] - y_vals[i] for i in range(len(y_vals)-1)]\n",
        "            features['vel_y_std'] = np.std(vel_y)\n",
        "            features['vel_y_max'] = max(vel_y)\n",
        "        else:\n",
        "            features['vel_y_std'] = 0\n",
        "            features['vel_y_max'] = features['avg_vel_y']\n",
        "        \n",
        "        features['start_y_norm'] = positions[0]['y'] / self.frame_h\n",
        "        features['end_y_norm'] = positions[-1]['y'] / self.frame_h\n",
        "        features['start_x_norm'] = positions[0]['x'] / self.frame_w\n",
        "        features['end_x_norm'] = positions[-1]['x'] / self.frame_w\n",
        "        \n",
        "        center_dist = [abs(p['x'] / self.frame_w - 0.5) for p in positions]\n",
        "        features['min_center_dist'] = min(center_dist)\n",
        "        features['avg_center_dist'] = np.mean(center_dist)\n",
        "        \n",
        "        first_f = positions[0]['features']\n",
        "        last_f = positions[-1]['features']\n",
        "        \n",
        "        features['first_area'] = first_f['area']\n",
        "        features['last_area'] = last_f['area']\n",
        "        features['area_growth'] = last_f['area'] / (first_f['area'] + 1)\n",
        "        features['first_intensity'] = first_f['intensity_mean']\n",
        "        features['last_intensity'] = last_f['intensity_mean']\n",
        "        features['intensity_change'] = last_f['intensity_mean'] - first_f['intensity_mean']\n",
        "        features['first_edge_density'] = first_f['edge_density']\n",
        "        features['last_edge_density'] = last_f['edge_density']\n",
        "        \n",
        "        areas = [p['features']['area'] for p in positions]\n",
        "        intensities = [p['features']['intensity_mean'] for p in positions]\n",
        "        edges = [p['features']['edge_density'] for p in positions]\n",
        "        \n",
        "        features['max_area'] = max(areas)\n",
        "        features['avg_intensity'] = np.mean(intensities)\n",
        "        features['intensity_std'] = np.std(intensities)\n",
        "        features['max_edge_density'] = max(edges)\n",
        "        features['avg_edge_density'] = np.mean(edges)\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def reset(self):\n",
        "        self.tracker = HazardTracker(max_age=20, min_hits=3)\n",
        "        self.frame_idx = 0\n",
        "        self.alert_active = False\n",
        "        self.alert_probability = 0.0\n",
        "        self.dangerous_tracks = []\n",
        "\n",
        "print(\"ApproachingHazardPredictor class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize tracked hazards and bump correlation\n",
        "def visualize_hazard_tracks(frames, labeled_tracks, bump_frames, n_examples=5):\n",
        "    \"\"\"visualize example tracks - bump-causing vs non-bump\"\"\"\n",
        "    bump_tracks = [t for t in labeled_tracks if t['caused_bump'] and t['hits'] >= 5]\n",
        "    safe_tracks = [t for t in labeled_tracks if not t['caused_bump'] and t['hits'] >= 5]\n",
        "    \n",
        "    print(f\"bump-causing tracks (5+ hits): {len(bump_tracks)}\")\n",
        "    print(f\"safe tracks (5+ hits): {len(safe_tracks)}\")\n",
        "    \n",
        "    #show examples of each\n",
        "    for label, tracks in [(\"BUMP-CAUSING\", bump_tracks[:n_examples]), (\"SAFE\", safe_tracks[:n_examples])]:\n",
        "        if not tracks:\n",
        "            continue\n",
        "        print(f\"\\n--- {label} HAZARD TRACKS ---\")\n",
        "        \n",
        "        for i, track in enumerate(tracks):\n",
        "            positions = track['positions']\n",
        "            start_frame = positions[0]['frame']\n",
        "            end_frame = positions[-1]['frame']\n",
        "            \n",
        "            #show start, middle, end frames\n",
        "            mid_frame = start_frame + (end_frame - start_frame) // 2\n",
        "            show_frames = [start_frame, mid_frame, end_frame]\n",
        "            \n",
        "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "            \n",
        "            for j, f_idx in enumerate(show_frames):\n",
        "                if f_idx >= len(frames):\n",
        "                    continue\n",
        "                frame = frames[f_idx].copy()\n",
        "                \n",
        "                #draw track trajectory up to this point\n",
        "                track_pts = [(p['x'], p['y']) for p in positions if p['frame'] <= f_idx]\n",
        "                for k in range(1, len(track_pts)):\n",
        "                    cv2.line(frame, \n",
        "                            (int(track_pts[k-1][0]), int(track_pts[k-1][1])),\n",
        "                            (int(track_pts[k][0]), int(track_pts[k][1])),\n",
        "                            (0, 0, 255) if label == \"BUMP-CAUSING\" else (0, 255, 0), 2)\n",
        "                \n",
        "                #mark current position\n",
        "                if track_pts:\n",
        "                    cx, cy = int(track_pts[-1][0]), int(track_pts[-1][1])\n",
        "                    cv2.circle(frame, (cx, cy), 8, (0, 0, 255) if label == \"BUMP-CAUSING\" else (0, 255, 0), -1)\n",
        "                \n",
        "                axes[j].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                axes[j].set_title(f\"frame {f_idx}\")\n",
        "                axes[j].axis('off')\n",
        "            \n",
        "            title = f\"{label} track {track['id']}: frames {start_frame}-{end_frame}, hits={track['hits']}\"\n",
        "            if track['caused_bump']:\n",
        "                title += f\", bump@{track['bump_frame']}\"\n",
        "            plt.suptitle(title)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "#visualize some example tracks\n",
        "if labeled_tracks:\n",
        "    visualize_hazard_tracks(frames, labeled_tracks, bump_frames, n_examples=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run demo: predict bumps from approaching hazards\n",
        "def run_hazard_prediction_demo(frames, model, scaler, feature_names, output_path=None):\n",
        "    \"\"\"run hazard-based bump prediction and optionally save video\"\"\"\n",
        "    if model is None:\n",
        "        print(\"no model available for prediction\")\n",
        "        return [], []\n",
        "    \n",
        "    predictor = ApproachingHazardPredictor(model, scaler, feature_names, frames[0].shape)\n",
        "    \n",
        "    h, w = frames[0].shape[:2]\n",
        "    out = None\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(str(output_path), fourcc, FPS, (w, h))\n",
        "    \n",
        "    alerts = []\n",
        "    probs = []\n",
        "    \n",
        "    for i, frame in enumerate(frames):\n",
        "        alert, prob, dangerous = predictor.process_frame(frame)\n",
        "        alerts.append(alert)\n",
        "        probs.append(prob)\n",
        "        \n",
        "        #annotate frame\n",
        "        frame_out = frame.copy()\n",
        "        \n",
        "        #draw all active tracks\n",
        "        for track in predictor.tracker.tracks:\n",
        "            if track['hits'] >= 3:\n",
        "                pts = [(p['x'], p['y']) for p in track['positions']]\n",
        "                for j in range(1, len(pts)):\n",
        "                    color = (0, 255, 255)  #yellow for tracked\n",
        "                    cv2.line(frame_out, (int(pts[j-1][0]), int(pts[j-1][1])),\n",
        "                            (int(pts[j][0]), int(pts[j][1])), color, 2)\n",
        "        \n",
        "        #highlight dangerous hazards\n",
        "        for d in dangerous:\n",
        "            x, y = int(d['position'][0]), int(d['position'][1])\n",
        "            p = d['probability']\n",
        "            cv2.circle(frame_out, (x, y), 15, (0, 0, 255), 3)\n",
        "            cv2.putText(frame_out, f\"{p:.0%}\", (x+10, y-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "        \n",
        "        #alert overlay\n",
        "        if alert:\n",
        "            cv2.rectangle(frame_out, (0, 0), (w-1, h-1), (0, 0, 255), 8)\n",
        "            cv2.putText(frame_out, f\"BUMP AHEAD! ({prob:.0%})\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
        "        else:\n",
        "            cv2.putText(frame_out, f\"Clear ({prob:.0%})\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "        \n",
        "        if out:\n",
        "            out.write(frame_out)\n",
        "        \n",
        "        if (i + 1) % 200 == 0:\n",
        "            print(f\"processed {i+1}/{len(frames)} frames\")\n",
        "    \n",
        "    if out:\n",
        "        out.release()\n",
        "    \n",
        "    return alerts, probs\n",
        "\n",
        "print(\"running hazard-based prediction demo...\")\n",
        "if rf_tracks is not None:\n",
        "    demo_alerts, demo_probs = run_hazard_prediction_demo(\n",
        "        frames, rf_tracks, scaler_tracks, feature_names_tracks,\n",
        "        OUTPUT_DIR / 'hazard_prediction_demo.mp4'\n",
        "    )\n",
        "    print(f\"\\ndemo complete! output: {OUTPUT_DIR / 'hazard_prediction_demo.mp4'}\")\n",
        "    print(f\"alerts triggered: {sum(demo_alerts)} frames\")\n",
        "else:\n",
        "    print(\"skipping demo - no model trained\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save hazard prediction model\n",
        "if rf_tracks is not None:\n",
        "    model_data = {\n",
        "        'model': rf_tracks,\n",
        "        'scaler': scaler_tracks,\n",
        "        'feature_names': feature_names_tracks,\n",
        "        'frame_shape': frames[0].shape,\n",
        "    }\n",
        "    joblib.dump(model_data, OUTPUT_DIR / 'hazard_predictor.joblib')\n",
        "    print(f\"hazard predictor saved to {OUTPUT_DIR / 'hazard_predictor.joblib'}\")\n",
        "\n",
        "#save track data for analysis\n",
        "track_data = {\n",
        "    'labeled_tracks': labeled_tracks,\n",
        "    'bump_frames': bump_frames,\n",
        "    'X_tracks': X_tracks,\n",
        "    'y_tracks': y_tracks,\n",
        "    'feature_names': feature_names_tracks,\n",
        "}\n",
        "with open(OUTPUT_DIR / 'hazard_track_data.pkl', 'wb') as f:\n",
        "    pickle.dump(track_data, f)\n",
        "print(f\"track data saved to {OUTPUT_DIR / 'hazard_track_data.pkl'}\")\n",
        "\n",
        "#summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HAZARD-BASED BUMP PREDICTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nvideo: {video_path.name}\")\n",
        "print(f\"frames: {len(frames)}, duration: {len(frames)/FPS:.1f}s\")\n",
        "\n",
        "print(f\"\\n--- detection ---\")\n",
        "print(f\"total hazards tracked: {hazard_tracker.track_id}\")\n",
        "print(f\"confirmed tracks (3+ hits): {len([t for t in labeled_tracks if t['hits'] >= 3])}\")\n",
        "print(f\"bump-causing tracks: {len([t for t in labeled_tracks if t['caused_bump']])}\")\n",
        "print(f\"bumps detected (ground truth): {len(bump_frames)}\")\n",
        "\n",
        "print(f\"\\n--- prediction model ---\")\n",
        "if rf_tracks is not None:\n",
        "    print(f\"training samples: {len(X_tracks)}\")\n",
        "    print(f\"features: {len(feature_names_tracks)}\")\n",
        "    print(f\"model: RandomForest\")\n",
        "else:\n",
        "    print(\"no model trained (insufficient data)\")\n",
        "\n",
        "print(f\"\\n--- key features for bump prediction ---\")\n",
        "if rf_tracks is not None:\n",
        "    for i in range(min(5, len(feature_names_tracks))):\n",
        "        print(f\"  {feature_names_tracks[indices[i]]}: {importances[indices[i]]:.3f}\")\n",
        "\n",
        "print(f\"\\n--- files saved ---\")\n",
        "print(f\"  {OUTPUT_DIR / 'hazard_predictor.joblib'}\")\n",
        "print(f\"  {OUTPUT_DIR / 'hazard_track_data.pkl'}\")\n",
        "print(f\"  {OUTPUT_DIR / 'hazard_prediction_demo.mp4'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Extract Visual Features for Prediction\n",
        "Now we look at 10-frame segments BEFORE bumps to identify visual patterns that predict bumps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_texture_features(frame):\n",
        "    \"\"\"extract texture features that might indicate cracks/bumps on sidewalk\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n",
        "    h, w = gray.shape\n",
        "    \n",
        "    features = {}\n",
        "    \n",
        "    #focus on bottom half (closer to scooter, more relevant)\n",
        "    roi = gray[h//2:, :]\n",
        "    \n",
        "    #edge detection (cracks have strong edges)\n",
        "    edges = cv2.Canny(roi, 50, 150)\n",
        "    features['edge_density'] = edges.mean() / 255\n",
        "    features['edge_std'] = edges.std() / 255\n",
        "    \n",
        "    #sobel gradients (horizontal lines = bumps/cracks)\n",
        "    sobel_x = cv2.Sobel(roi, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobel_y = cv2.Sobel(roi, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    features['sobel_x_mean'] = np.abs(sobel_x).mean()\n",
        "    features['sobel_y_mean'] = np.abs(sobel_y).mean()\n",
        "    features['sobel_ratio'] = features['sobel_y_mean'] / (features['sobel_x_mean'] + 1e-6)\n",
        "    \n",
        "    #laplacian (overall texture/roughness)\n",
        "    laplacian = cv2.Laplacian(roi, cv2.CV_64F)\n",
        "    features['laplacian_var'] = laplacian.var()\n",
        "    features['laplacian_mean'] = np.abs(laplacian).mean()\n",
        "    \n",
        "    #histogram features (contrast)\n",
        "    features['intensity_mean'] = roi.mean()\n",
        "    features['intensity_std'] = roi.std()\n",
        "    \n",
        "    #gabor filter for texture (detects linear patterns like cracks)\n",
        "    gabor_responses = []\n",
        "    for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:\n",
        "        kernel = cv2.getGaborKernel((21, 21), 5, theta, 10, 0.5, 0)\n",
        "        filtered = cv2.filter2D(roi, cv2.CV_64F, kernel)\n",
        "        gabor_responses.append(np.abs(filtered).mean())\n",
        "    features['gabor_0'] = gabor_responses[0]\n",
        "    features['gabor_45'] = gabor_responses[1]\n",
        "    features['gabor_90'] = gabor_responses[2]\n",
        "    features['gabor_135'] = gabor_responses[3]\n",
        "    features['gabor_max'] = max(gabor_responses)\n",
        "    \n",
        "    return features\n",
        "\n",
        "#test on single frame\n",
        "test_features = extract_texture_features(frames[0])\n",
        "print(\"texture features:\")\n",
        "for k, v in test_features.items():\n",
        "    print(f\"  {k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_segment_features(frames, flows, start_idx, segment_length=10):\n",
        "    \"\"\"extract features from a segment of frames for bump prediction\"\"\"\n",
        "    end_idx = min(start_idx + segment_length, len(frames))\n",
        "    segment_frames = frames[start_idx:end_idx]\n",
        "    segment_flows = flows[start_idx:min(end_idx, len(flows))]\n",
        "    \n",
        "    if len(segment_frames) < segment_length // 2:\n",
        "        return None\n",
        "    \n",
        "    features = {}\n",
        "    \n",
        "    #aggregate texture features across segment\n",
        "    texture_feats = defaultdict(list)\n",
        "    for frame in segment_frames:\n",
        "        tf = extract_texture_features(frame)\n",
        "        for k, v in tf.items():\n",
        "            texture_feats[k].append(v)\n",
        "    \n",
        "    for k, v in texture_feats.items():\n",
        "        features[f'tex_{k}_mean'] = np.mean(v)\n",
        "        features[f'tex_{k}_std'] = np.std(v)\n",
        "        features[f'tex_{k}_max'] = np.max(v)\n",
        "        features[f'tex_{k}_trend'] = v[-1] - v[0] if len(v) > 1 else 0\n",
        "    \n",
        "    #optical flow features in segment\n",
        "    if len(segment_flows) > 0:\n",
        "        vy_vals = [flow[..., 1].mean() for flow in segment_flows]\n",
        "        vx_vals = [flow[..., 0].mean() for flow in segment_flows]\n",
        "        mag_vals = [np.sqrt(flow[..., 0]**2 + flow[..., 1]**2).mean() for flow in segment_flows]\n",
        "        \n",
        "        features['flow_vy_mean'] = np.mean(vy_vals)\n",
        "        features['flow_vy_std'] = np.std(vy_vals)\n",
        "        features['flow_vy_trend'] = vy_vals[-1] - vy_vals[0] if len(vy_vals) > 1 else 0\n",
        "        features['flow_vx_mean'] = np.mean(vx_vals)\n",
        "        features['flow_mag_mean'] = np.mean(mag_vals)\n",
        "        features['flow_mag_max'] = np.max(mag_vals)\n",
        "    \n",
        "    return features\n",
        "\n",
        "#test\n",
        "test_seg_features = extract_segment_features(frames, flows, 0, SEGMENT_LENGTH)\n",
        "print(f\"segment features: {len(test_seg_features)} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create training data: segments with labels indicating if bump occurs in next N frames\n",
        "def create_training_data(frames, flows, bump_frames, segment_length=10, lookahead=10):\n",
        "    \"\"\"create training dataset for bump prediction\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    segment_indices = []\n",
        "    \n",
        "    bump_set = set(bump_frames)\n",
        "    \n",
        "    #slide window through video\n",
        "    step = segment_length // 2  #50% overlap\n",
        "    \n",
        "    for start_idx in range(0, len(frames) - segment_length - lookahead, step):\n",
        "        #extract features from current segment\n",
        "        features = extract_segment_features(frames, flows, start_idx, segment_length)\n",
        "        \n",
        "        if features is None:\n",
        "            continue\n",
        "        \n",
        "        #check if bump occurs in the lookahead window (after segment ends)\n",
        "        lookahead_start = start_idx + segment_length\n",
        "        lookahead_end = lookahead_start + lookahead\n",
        "        has_bump = any(bf in range(lookahead_start, lookahead_end) for bf in bump_set)\n",
        "        \n",
        "        X.append(list(features.values()))\n",
        "        y.append(1 if has_bump else 0)\n",
        "        segment_indices.append(start_idx)\n",
        "        \n",
        "        if len(X) % 50 == 0:\n",
        "            print(f\"processed {len(X)} segments...\")\n",
        "    \n",
        "    feature_names = list(features.keys()) if features else []\n",
        "    return np.array(X), np.array(y), segment_indices, feature_names\n",
        "\n",
        "print(\"creating training data...\")\n",
        "X, y, segment_indices, feature_names = create_training_data(\n",
        "    frames, flows, bump_frames, \n",
        "    segment_length=SEGMENT_LENGTH, \n",
        "    lookahead=LOOKAHEAD_FRAMES\n",
        ")\n",
        "\n",
        "print(f\"\\ntraining data shape: X={X.shape}, y={y.shape}\")\n",
        "print(f\"positive samples (bump ahead): {y.sum()} ({100*y.mean():.1f}%)\")\n",
        "print(f\"negative samples (no bump): {(1-y).sum()}\")\n",
        "print(f\"feature count: {len(feature_names)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Train Predictive Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#handle class imbalance and train model\n",
        "#split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "#scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#compute class weights for imbalanced data\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "print(f\"class weights: {class_weight_dict}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train random forest model\n",
        "print(\"training random forest...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    class_weight=class_weight_dict,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "#cross-validation\n",
        "cv_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "print(f\"cross-validation F1 scores: {cv_scores}\")\n",
        "print(f\"mean CV F1: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train gradient boosting model\n",
        "print(\"training gradient boosting...\")\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "cv_scores_gb = cross_val_score(gb_model, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "print(f\"GB cross-validation F1 scores: {cv_scores_gb}\")\n",
        "print(f\"mean CV F1: {cv_scores_gb.mean():.3f} (+/- {cv_scores_gb.std()*2:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#evaluate both models\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{model_name} evaluation:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(\"\\nclassification report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['no bump', 'bump']))\n",
        "    \n",
        "    print(\"confusion matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    \n",
        "    return y_pred, y_proba\n",
        "\n",
        "rf_pred, rf_proba = evaluate_model(rf_model, X_test_scaled, y_test, \"Random Forest\")\n",
        "gb_pred, gb_proba = evaluate_model(gb_model, X_test_scaled, y_test, \"Gradient Boosting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#feature importance analysis\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_n = min(20, len(feature_names))\n",
        "plt.barh(range(top_n), importances[indices[:top_n]][::-1])\n",
        "plt.yticks(range(top_n), [feature_names[i] for i in indices[:top_n]][::-1])\n",
        "plt.xlabel('feature importance')\n",
        "plt.title('top features for bump prediction')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\ntop 10 most important features:\")\n",
        "for i in range(min(10, len(feature_names))):\n",
        "    print(f\"  {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#precision-recall curve\n",
        "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(y_test, rf_proba)\n",
        "precision_gb, recall_gb, thresholds_gb = precision_recall_curve(y_test, gb_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall_rf, precision_rf, 'b-', label='random forest')\n",
        "plt.plot(recall_gb, precision_gb, 'r-', label='gradient boosting')\n",
        "plt.xlabel('recall')\n",
        "plt.ylabel('precision')\n",
        "plt.title('precision-recall curve for bump prediction')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Save Model and Create Alert System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save best model\n",
        "best_model = rf_model  #choose based on evaluation\n",
        "\n",
        "model_data = {\n",
        "    'model': best_model,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': feature_names,\n",
        "    'segment_length': SEGMENT_LENGTH,\n",
        "    'lookahead': LOOKAHEAD_FRAMES\n",
        "}\n",
        "\n",
        "joblib.dump(model_data, OUTPUT_DIR / 'bump_predictor.joblib')\n",
        "print(f\"model saved to {OUTPUT_DIR / 'bump_predictor.joblib'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save bump detection results\n",
        "detection_results = {\n",
        "    'bump_frames': bump_frames,\n",
        "    'bump_intensities': bump_intensities,\n",
        "    'bump_threshold': bump_threshold,\n",
        "    'bump_labels': bump_labels,\n",
        "    'flow_features': flow_features,\n",
        "    'fps': FPS\n",
        "}\n",
        "\n",
        "with open(OUTPUT_DIR / 'bump_detection.pkl', 'wb') as f:\n",
        "    pickle.dump(detection_results, f)\n",
        "    \n",
        "print(f\"detection results saved to {OUTPUT_DIR / 'bump_detection.pkl'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BumpAlertSystem:\n",
        "    \"\"\"real-time bump prediction alert system\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path):\n",
        "        data = joblib.load(model_path)\n",
        "        self.model = data['model']\n",
        "        self.scaler = data['scaler']\n",
        "        self.feature_names = data['feature_names']\n",
        "        self.segment_length = data['segment_length']\n",
        "        self.lookahead = data['lookahead']\n",
        "        \n",
        "        #buffer for frames and flows\n",
        "        self.frame_buffer = []\n",
        "        self.flow_buffer = []\n",
        "        self.prev_gray = None\n",
        "        \n",
        "        #alert state\n",
        "        self.alert_active = False\n",
        "        self.alert_probability = 0.0\n",
        "        \n",
        "    def process_frame(self, frame):\n",
        "        \"\"\"process new frame and return alert status\"\"\"\n",
        "        self.frame_buffer.append(frame)\n",
        "        \n",
        "        #compute optical flow if we have previous frame\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        if self.prev_gray is not None:\n",
        "            flow = cv2.calcOpticalFlowFarneback(\n",
        "                self.prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "            )\n",
        "            self.flow_buffer.append(flow)\n",
        "        self.prev_gray = gray\n",
        "        \n",
        "        #keep buffer at segment length\n",
        "        if len(self.frame_buffer) > self.segment_length:\n",
        "            self.frame_buffer.pop(0)\n",
        "        if len(self.flow_buffer) > self.segment_length:\n",
        "            self.flow_buffer.pop(0)\n",
        "        \n",
        "        #predict if we have enough data\n",
        "        if len(self.frame_buffer) >= self.segment_length and len(self.flow_buffer) >= self.segment_length - 1:\n",
        "            features = extract_segment_features(\n",
        "                np.array(self.frame_buffer), \n",
        "                self.flow_buffer, \n",
        "                0, \n",
        "                self.segment_length\n",
        "            )\n",
        "            \n",
        "            if features:\n",
        "                X = np.array([list(features.values())])\n",
        "                X_scaled = self.scaler.transform(X)\n",
        "                \n",
        "                prob = self.model.predict_proba(X_scaled)[0, 1]\n",
        "                self.alert_probability = prob\n",
        "                self.alert_active = prob > 0.5\n",
        "        \n",
        "        return self.alert_active, self.alert_probability\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"reset buffer state\"\"\"\n",
        "        self.frame_buffer = []\n",
        "        self.flow_buffer = []\n",
        "        self.prev_gray = None\n",
        "        self.alert_active = False\n",
        "        self.alert_probability = 0.0\n",
        "\n",
        "print(\"BumpAlertSystem class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#demo: run alert system on video and visualize\n",
        "def demo_alert_system(video_path, model_path, output_video_path=None):\n",
        "    \"\"\"run bump alert system on video with visualization\"\"\"\n",
        "    alert_system = BumpAlertSystem(model_path)\n",
        "    \n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "    #setup output video\n",
        "    out = None\n",
        "    if output_video_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))\n",
        "    \n",
        "    alerts = []\n",
        "    probabilities = []\n",
        "    frame_idx = 0\n",
        "    \n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        alert, prob = alert_system.process_frame(frame)\n",
        "        alerts.append(alert)\n",
        "        probabilities.append(prob)\n",
        "        \n",
        "        #add visual alert to frame\n",
        "        if alert:\n",
        "            #red border for alert\n",
        "            cv2.rectangle(frame, (0, 0), (width-1, height-1), (0, 0, 255), 10)\n",
        "            cv2.putText(frame, f\"BUMP AHEAD! ({prob:.0%})\", (10, 30), \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        else:\n",
        "            cv2.putText(frame, f\"Clear ({prob:.0%})\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "        \n",
        "        if out:\n",
        "            out.write(frame)\n",
        "        \n",
        "        frame_idx += 1\n",
        "        if frame_idx % 100 == 0:\n",
        "            print(f\"processed {frame_idx} frames...\")\n",
        "    \n",
        "    cap.release()\n",
        "    if out:\n",
        "        out.release()\n",
        "    \n",
        "    return alerts, probabilities\n",
        "\n",
        "#run demo\n",
        "if video_files:\n",
        "    print(\"running alert system demo...\")\n",
        "    alerts, probs = demo_alert_system(\n",
        "        video_files[0], \n",
        "        OUTPUT_DIR / 'bump_predictor.joblib',\n",
        "        OUTPUT_DIR / 'bump_alert_demo.mp4'\n",
        "    )\n",
        "    print(f\"\\ndemo complete! output saved to {OUTPUT_DIR / 'bump_alert_demo.mp4'}\")\n",
        "    print(f\"total alerts triggered: {sum(alerts)} frames\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize alert predictions over time\n",
        "if 'probs' in dir():\n",
        "    time_probs = np.arange(len(probs)) / FPS\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 6), sharex=True)\n",
        "    \n",
        "    #probability over time\n",
        "    axes[0].plot(time_probs, probs, 'b-', alpha=0.7)\n",
        "    axes[0].axhline(y=0.5, color='r', linestyle='--', label='alert threshold')\n",
        "    axes[0].fill_between(time_probs, 0, probs, where=np.array(probs) > 0.5, \n",
        "                         color='red', alpha=0.3, label='alert active')\n",
        "    axes[0].set_ylabel('bump probability')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    #compare with actual bumps\n",
        "    axes[1].plot(time_signal, bump_signal, 'g-', alpha=0.7, label='actual bump signal')\n",
        "    axes[1].axhline(y=bump_threshold, color='orange', linestyle='--', label='detection threshold')\n",
        "    axes[1].set_xlabel('time (s)')\n",
        "    axes[1].set_ylabel('bump intensity')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('bump prediction vs actual bumps')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Visualize Bump-Causing Features\n",
        "Look at frames before bumps to see what visual patterns predict them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize frames before and during bumps\n",
        "def visualize_bump_sequence(frames, flows, bump_frame, pre_frames=10, post_frames=5):\n",
        "    \"\"\"show frames leading up to and including a bump\"\"\"\n",
        "    start = max(0, bump_frame - pre_frames)\n",
        "    end = min(len(frames), bump_frame + post_frames + 1)\n",
        "    \n",
        "    sequence = frames[start:end]\n",
        "    n_frames = len(sequence)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, n_frames, figsize=(2*n_frames, 5))\n",
        "    \n",
        "    for i, frame in enumerate(sequence):\n",
        "        frame_idx = start + i\n",
        "        \n",
        "        #original frame\n",
        "        axes[0, i].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        title = f\"t={frame_idx}\"\n",
        "        if frame_idx == bump_frame:\n",
        "            title += \" (BUMP)\"\n",
        "            axes[0, i].set_title(title, color='red', fontweight='bold')\n",
        "        else:\n",
        "            axes[0, i].set_title(title)\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        #edge detection (shows cracks)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        edges = cv2.Canny(gray, 50, 150)\n",
        "        axes[1, i].imshow(edges, cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    axes[0, 0].set_ylabel('original')\n",
        "    axes[1, 0].set_ylabel('edges')\n",
        "    \n",
        "    plt.suptitle(f'bump at frame {bump_frame}')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "#visualize first few bumps\n",
        "n_bumps_to_show = min(3, len(bump_frames))\n",
        "for i, bf in enumerate(bump_frames[:n_bumps_to_show]):\n",
        "    print(f\"\\nbump {i+1} at frame {bf} (time: {bf/FPS:.2f}s)\")\n",
        "    visualize_bump_sequence(frames, flows, bf)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#analyze texture features before vs during/after bumps\n",
        "def compare_bump_features(frames, bump_frames, pre_window=10):\n",
        "    \"\"\"compare texture features before bumps vs random segments\"\"\"\n",
        "    bump_features = []\n",
        "    normal_features = []\n",
        "    \n",
        "    #features before bumps\n",
        "    for bf in bump_frames:\n",
        "        start = max(0, bf - pre_window)\n",
        "        for frame in frames[start:bf]:\n",
        "            bump_features.append(extract_texture_features(frame))\n",
        "    \n",
        "    #random non-bump segments\n",
        "    bump_set = set()\n",
        "    for bf in bump_frames:\n",
        "        bump_set.update(range(max(0, bf-pre_window), min(len(frames), bf+5)))\n",
        "    \n",
        "    normal_indices = [i for i in range(len(frames)) if i not in bump_set]\n",
        "    np.random.seed(42)\n",
        "    sample_indices = np.random.choice(normal_indices, size=min(len(normal_indices), len(bump_features)), replace=False)\n",
        "    \n",
        "    for idx in sample_indices:\n",
        "        normal_features.append(extract_texture_features(frames[idx]))\n",
        "    \n",
        "    return bump_features, normal_features\n",
        "\n",
        "if len(bump_frames) > 0:\n",
        "    print(\"analyzing texture differences...\")\n",
        "    bump_feats, normal_feats = compare_bump_features(frames, bump_frames)\n",
        "    \n",
        "    #convert to arrays for comparison\n",
        "    feature_keys = list(bump_feats[0].keys())\n",
        "    bump_arr = np.array([[f[k] for k in feature_keys] for f in bump_feats])\n",
        "    normal_arr = np.array([[f[k] for k in feature_keys] for f in normal_feats])\n",
        "    \n",
        "    #compare means\n",
        "    print(\"\\nfeature comparison (before bump vs normal):\")\n",
        "    print(f\"{'feature':<20} {'bump mean':>12} {'normal mean':>12} {'diff':>12}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for i, key in enumerate(feature_keys):\n",
        "        bump_mean = bump_arr[:, i].mean()\n",
        "        normal_mean = normal_arr[:, i].mean()\n",
        "        diff = bump_mean - normal_mean\n",
        "        diff_pct = 100 * diff / (normal_mean + 1e-6)\n",
        "        print(f\"{key:<20} {bump_mean:>12.4f} {normal_mean:>12.4f} {diff_pct:>+10.1f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#summary\n",
        "print(\"=\"*60)\n",
        "print(\"BUMP DETECTION & PREDICTION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nvideo analyzed: {video_path.name if video_files else 'N/A'}\")\n",
        "print(f\"total frames: {len(frames)}\")\n",
        "print(f\"video duration: {len(frames)/FPS:.1f} seconds\")\n",
        "print(f\"\\nbumps detected: {len(bump_frames)}\")\n",
        "print(f\"bump detection threshold: {bump_threshold:.3f}\")\n",
        "print(f\"\\nmodel trained on: {len(X)} segments\")\n",
        "print(f\"segment length: {SEGMENT_LENGTH} frames\")\n",
        "print(f\"lookahead window: {LOOKAHEAD_FRAMES} frames\")\n",
        "print(f\"\\nmodel saved to: {OUTPUT_DIR / 'bump_predictor.joblib'}\")\n",
        "print(f\"\\nto use the alert system:\")\n",
        "print(\"  alert_system = BumpAlertSystem('output/bump_predictor.joblib')\")\n",
        "print(\"  alert, prob = alert_system.process_frame(frame)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
